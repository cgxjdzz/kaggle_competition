{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "363892f8",
   "metadata": {
    "papermill": {
     "duration": 0.004454,
     "end_time": "2022-10-09T15:24:28.139945",
     "exception": false,
     "start_time": "2022-10-09T15:24:28.135491",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# OpenAI CLIP\n",
    "Clip from [huggingface](https://huggingface.co/docs/transformers/model_doc/clip) is not jit friendly and doesn't work very well with this competition format.\n",
    "\n",
    "However, OpenAI made available their own jitted weights on their repo https://github.com/openai/CLIP\n",
    "\n",
    "This notebook briefly shows how to import it and use it on a simple model ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cfdce266",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_kg_hide-output": true,
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-10-09T15:24:28.147815Z",
     "iopub.status.busy": "2022-10-09T15:24:28.147421Z",
     "iopub.status.idle": "2022-10-09T15:24:44.718214Z",
     "shell.execute_reply": "2022-10-09T15:24:44.717058Z"
    },
    "papermill": {
     "duration": 16.577284,
     "end_time": "2022-10-09T15:24:44.720637",
     "exception": false,
     "start_time": "2022-10-09T15:24:28.143353",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/openai/CLIP.git\r\n",
      "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-8rw11k0m\r\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-8rw11k0m\r\n",
      "  Resolved https://github.com/openai/CLIP.git to commit d50d76daa670286dd6cacf3bcd80b5e4823fc8e1\r\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l-\b \bdone\r\n",
      "\u001b[?25hCollecting timm\r\n",
      "  Downloading timm-0.6.11-py3-none-any.whl (548 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m548.7/548.7 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting torchinfo\r\n",
      "  Downloading torchinfo-1.7.1-py3-none-any.whl (22 kB)\r\n",
      "Requirement already satisfied: huggingface-hub in /opt/conda/lib/python3.7/site-packages (from timm) (0.8.1)\r\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.7/site-packages (from timm) (6.0)\r\n",
      "Requirement already satisfied: torch>=1.7 in /opt/conda/lib/python3.7/site-packages (from timm) (1.11.0)\r\n",
      "Requirement already satisfied: torchvision in /opt/conda/lib/python3.7/site-packages (from timm) (0.12.0)\r\n",
      "Collecting ftfy\r\n",
      "  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: regex in /opt/conda/lib/python3.7/site-packages (from clip==1.0) (2021.11.10)\r\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from clip==1.0) (4.64.0)\r\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch>=1.7->timm) (4.1.1)\r\n",
      "Requirement already satisfied: wcwidth>=0.2.5 in /opt/conda/lib/python3.7/site-packages (from ftfy->clip==1.0) (0.2.5)\r\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from huggingface-hub->timm) (2.28.1)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from huggingface-hub->timm) (3.7.1)\r\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from huggingface-hub->timm) (4.12.0)\r\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub->timm) (21.3)\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torchvision->timm) (1.21.6)\r\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.7/site-packages (from torchvision->timm) (9.1.1)\r\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=20.9->huggingface-hub->timm) (3.0.9)\r\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->huggingface-hub->timm) (3.8.0)\r\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests->huggingface-hub->timm) (2.1.0)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->huggingface-hub->timm) (3.3)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->huggingface-hub->timm) (1.26.9)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->huggingface-hub->timm) (2022.6.15)\r\n",
      "Building wheels for collected packages: clip\r\n",
      "  Building wheel for clip (setup.py) ... \u001b[?25l-\b \b\\\b \bdone\r\n",
      "\u001b[?25h  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369409 sha256=093d0aac3740305725cd05c586c0e5d20055e207cef7361e7793bd9b15f9c719\r\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-k1xw_uvf/wheels/fd/b9/c3/5b4470e35ed76e174bff77c92f91da82098d5e35fd5bc8cdac\r\n",
      "Successfully built clip\r\n",
      "Installing collected packages: torchinfo, ftfy, timm, clip\r\n",
      "Successfully installed clip-1.0 ftfy-6.1.1 timm-0.6.11 torchinfo-1.7.1\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install timm torchinfo git+https://github.com/openai/CLIP.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "360484a5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-09T15:24:44.732055Z",
     "iopub.status.busy": "2022-10-09T15:24:44.731746Z",
     "iopub.status.idle": "2022-10-09T15:25:44.219811Z",
     "shell.execute_reply": "2022-10-09T15:25:44.218788Z"
    },
    "papermill": {
     "duration": 59.496293,
     "end_time": "2022-10-09T15:25:44.222285",
     "exception": false,
     "start_time": "2022-10-09T15:24:44.725992",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 891M/891M [00:46<00:00, 20.0MiB/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchinfo import summary\n",
    "from torchvision import transforms\n",
    "import pickle\n",
    "import timm\n",
    "from timm.data import resolve_data_config\n",
    "\n",
    "import clip\n",
    "from clip.clip import _download, _MODELS\n",
    "\n",
    "clip_code = 'ViT-L/14@336px'\n",
    "model_path = _download(_MODELS[clip_code], os.path.expanduser(\"~/.cache/clip\"))\n",
    "with open(model_path, 'rb') as opened_file:\n",
    "    clip_vit = torch.jit.load(opened_file, map_location=\"cuda:0\").visual.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "390df413",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-09T15:25:44.249393Z",
     "iopub.status.busy": "2022-10-09T15:25:44.247635Z",
     "iopub.status.idle": "2022-10-09T15:25:44.252898Z",
     "shell.execute_reply": "2022-10-09T15:25:44.252072Z"
    },
    "papermill": {
     "duration": 0.020422,
     "end_time": "2022-10-09T15:25:44.255020",
     "exception": false,
     "start_time": "2022-10-09T15:25:44.234598",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# timm.list_models(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dded50ce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-09T15:25:44.279555Z",
     "iopub.status.busy": "2022-10-09T15:25:44.279286Z",
     "iopub.status.idle": "2022-10-09T15:25:45.067184Z",
     "shell.execute_reply": "2022-10-09T15:25:45.065888Z"
    },
    "papermill": {
     "duration": 0.803328,
     "end_time": "2022-10-09T15:25:45.069943",
     "exception": false,
     "start_time": "2022-10-09T15:25:44.266615",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "N_DROP_CONV = 0\n",
    "N_DROP_CLIP = 0\n",
    "\n",
    "with open('../input/1009-pca-model/clip-vit.pickle', 'rb') as r:\n",
    "    pca_clip = pickle.load(r)\n",
    "with open('../input/1009-pca-model/convnext22k (1).pickle', 'rb') as r:\n",
    "    pca_conv = pickle.load(r)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0cd82647",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-09T15:25:45.095034Z",
     "iopub.status.busy": "2022-10-09T15:25:45.094741Z",
     "iopub.status.idle": "2022-10-09T15:25:45.107358Z",
     "shell.execute_reply": "2022-10-09T15:25:45.106024Z"
    },
    "papermill": {
     "duration": 0.027885,
     "end_time": "2022-10-09T15:25:45.109824",
     "exception": false,
     "start_time": "2022-10-09T15:25:45.081939",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BlendModel(nn.Module):\n",
    "    \"\"\" Blend CLIP with another model from TIMM \"\"\"\n",
    "    def __init__(self, model2, model2weight, normalize):\n",
    "        super().__init__()\n",
    "        self.clip = clip_vit\n",
    "        self.clip_norm = nn.Sequential(\n",
    "            transforms.Resize(size=[336, 336]),\n",
    "            transforms.Normalize(mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711]),\n",
    "        )\n",
    "        \n",
    "        self.N_DROP_CONV = N_DROP_CONV\n",
    "        self.N_DROP_CLIP = N_DROP_CLIP\n",
    "        \n",
    "        self.pca_mean_clip = torch.nn.Parameter(torch.tensor(pca_clip.mean_))\n",
    "        self.pca_matrix_clip = torch.nn.Parameter(torch.tensor(pca_clip.components_))\n",
    "        \n",
    "        self.pca_mean_conv = torch.nn.Parameter(torch.tensor(pca_conv.mean_))\n",
    "        self.pca_matrix_conv = torch.nn.Parameter(torch.tensor(pca_conv.components_))\n",
    "\n",
    "        self.model2 = timm.create_model(model2, pretrained=True, num_classes=0)\n",
    "        config = resolve_data_config({}, model=self.model2)\n",
    "        print(model2, config)\n",
    "        self.model2_norm = nn.Sequential(\n",
    "            transforms.Resize(size=config['input_size'][-2:]),\n",
    "#             transforms.Normalize(mean=config['mean'], std=config['std']),\n",
    "            ## force clip norm params instead\n",
    "            transforms.Normalize(mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711]),\n",
    "        )\n",
    "        self.model2weight = model2weight\n",
    "        self.normalize = normalize\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.clip_norm(x / 255.0)\n",
    "        x1 = (self.clip(x1.half()))\n",
    "        x1 = torch.nn.functional.normalize(x1)\n",
    "        x1 = (x1 - self.pca_mean_clip) @ (self.pca_matrix_clip).T\n",
    "        x2 = self.model2_norm(x / 255.0)\n",
    "        x2 = (self.model2(x2))\n",
    "        x2 = torch.nn.functional.normalize(x2)\n",
    "        x2 = (x2 - self.pca_mean_conv) @ (self.pca_matrix_conv).T\n",
    "        x1 = x1[:, self.N_DROP_CLIP : self.N_DROP_CLIP + 64]\n",
    "        print(x1.shape)\n",
    "        x2 = x2[:, self.N_DROP_CONV : self.N_DROP_CONV + 64]\n",
    "        \n",
    "        if self.normalize:\n",
    "            return F.normalize(x1) + F.normalize(x2) * self.model2weight\n",
    "        else:\n",
    "            return x1 * (1 - self.model2weight) + x2 * self.model2weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f83f8e22",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-09T15:25:45.134769Z",
     "iopub.status.busy": "2022-10-09T15:25:45.134510Z",
     "iopub.status.idle": "2022-10-09T15:27:09.634950Z",
     "shell.execute_reply": "2022-10-09T15:27:09.633175Z"
    },
    "papermill": {
     "duration": 84.516679,
     "end_time": "2022-10-09T15:27:09.638699",
     "exception": false,
     "start_time": "2022-10-09T15:25:45.122020",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://dl.fbaipublicfiles.com/convnext/convnext_xlarge_22k_224.pth\" to /root/.cache/torch/hub/checkpoints/convnext_xlarge_22k_224.pth\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convnext_xlarge_in22k {'input_size': (3, 224, 224), 'interpolation': 'bicubic', 'mean': (0.485, 0.456, 0.406), 'std': (0.229, 0.224, 0.225), 'crop_pct': 0.875}\n",
      "torch.Size([1, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py:1110: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /usr/local/src/pytorch/aten/src/ATen/native/BinaryOps.cpp:601.)\n",
      "  return forward_call(*input, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "=========================================================================================================\n",
       "Layer (type:depth-idx)                                  Output Shape              Param #\n",
       "=========================================================================================================\n",
       "├─VisualTransformer: 1-1                                --                        1,378,304\n",
       "│    └─Conv2d: 2-1                                      --                        602,112\n",
       "│    └─LayerNorm: 2-2                                   --                        2,048\n",
       "│    └─Transformer: 2-3                                 --                        --\n",
       "│    │    └─Sequential: 3-1                             --                        302,309,376\n",
       "│    └─LayerNorm: 2-4                                   --                        2,048\n",
       "BlendModel                                              [1, 64]                   304,476,928\n",
       "├─Sequential: 1-2                                       [1, 3, 336, 336]          --\n",
       "│    └─Resize: 2-5                                      [1, 3, 336, 336]          --\n",
       "│    └─Normalize: 2-6                                   [1, 3, 336, 336]          --\n",
       "├─Sequential: 1-3                                       [1, 3, 224, 224]          --\n",
       "│    └─Resize: 2-7                                      [1, 3, 224, 224]          --\n",
       "│    └─Normalize: 2-8                                   [1, 3, 224, 224]          --\n",
       "├─ConvNeXt: 1-4                                         [1, 2048]                 --\n",
       "│    └─Sequential: 2-9                                  [1, 256, 56, 56]          --\n",
       "│    │    └─Conv2d: 3-2                                 [1, 256, 56, 56]          12,544\n",
       "│    │    └─LayerNorm2d: 3-3                            [1, 256, 56, 56]          512\n",
       "│    └─Sequential: 2-10                                 [1, 2048, 7, 7]           --\n",
       "│    │    └─ConvNeXtStage: 3-4                          [1, 256, 56, 56]          1,617,408\n",
       "│    │    └─ConvNeXtStage: 3-5                          [1, 512, 28, 28]          6,905,856\n",
       "│    │    └─ConvNeXtStage: 3-6                          [1, 1024, 14, 14]         230,195,200\n",
       "│    │    └─ConvNeXtStage: 3-7                          [1, 2048, 7, 7]           109,412,352\n",
       "│    └─Identity: 2-11                                   [1, 2048, 7, 7]           --\n",
       "│    └─Sequential: 2-12                                 --                        --\n",
       "│    │    └─SelectAdaptivePool2d: 3-8                   [1, 2048, 1, 1]           --\n",
       "│    │    └─LayerNorm2d: 3-9                            [1, 2048, 1, 1]           4,096\n",
       "│    │    └─Flatten: 3-10                               [1, 2048]                 --\n",
       "│    │    └─Dropout: 3-11                               [1, 2048]                 --\n",
       "│    │    └─Identity: 3-12                              [1, 2048]                 --\n",
       "=========================================================================================================\n",
       "Total params: 956,918,784\n",
       "Trainable params: 956,918,784\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 2.08\n",
       "=========================================================================================================\n",
       "Input size (MB): 3.69\n",
       "Forward/backward pass size (MB): 552.35\n",
       "Params size (MB): 1847.34\n",
       "Estimated Total Size (MB): 2403.38\n",
       "========================================================================================================="
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BlendModel(model2='convnext_xlarge_in22k', model2weight=0.2, normalize=False).to('cuda')\n",
    "# model = BlendModel(model2='convnext_xlarge_384_in22ft1k', model2weight=0.82, normalize=False)\n",
    "model.eval()\n",
    "\n",
    "test_input_size = (1, 3, 480, 640)  # the model should work with any input_size\n",
    "summary(model, input_size=test_input_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "30df5988",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-09T15:27:09.665719Z",
     "iopub.status.busy": "2022-10-09T15:27:09.664145Z",
     "iopub.status.idle": "2022-10-09T15:27:14.896207Z",
     "shell.execute_reply": "2022-10-09T15:27:14.895172Z"
    },
    "papermill": {
     "duration": 5.247341,
     "end_time": "2022-10-09T15:27:14.898622",
     "exception": false,
     "start_time": "2022-10-09T15:27:09.651281",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "saved_model = torch.jit.script(model)\n",
    "del model\n",
    "\n",
    "saved_model.save('saved_model.pt')\n",
    "del saved_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8426578",
   "metadata": {
    "papermill": {
     "duration": 0.013137,
     "end_time": "2022-10-09T15:27:21.907572",
     "exception": false,
     "start_time": "2022-10-09T15:27:21.894435",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Verify saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "771cbcfd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-09T15:27:21.933249Z",
     "iopub.status.busy": "2022-10-09T15:27:21.932858Z",
     "iopub.status.idle": "2022-10-09T15:27:24.575835Z",
     "shell.execute_reply": "2022-10-09T15:27:24.574498Z"
    },
    "papermill": {
     "duration": 2.659121,
     "end_time": "2022-10-09T15:27:24.578873",
     "exception": false,
     "start_time": "2022-10-09T15:27:21.919752",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 64]\n",
      "torch.float64 torch.Size([1, 64]) -0.12764165663020005\n"
     ]
    }
   ],
   "source": [
    "saved_model = torch.jit.load('saved_model.pt').cuda()\n",
    "input = torch.randint(0, 255, test_input_size, device='cuda', dtype=torch.uint8)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = saved_model(input)\n",
    "    print(output.dtype, output.shape, output.mean().item())\n",
    "    assert output.shape == torch.Size([test_input_size[0], 64])\n",
    "    torch.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4f18d2c4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-09T15:27:24.606413Z",
     "iopub.status.busy": "2022-10-09T15:27:24.606050Z",
     "iopub.status.idle": "2022-10-09T15:27:24.610446Z",
     "shell.execute_reply": "2022-10-09T15:27:24.609423Z"
    },
    "papermill": {
     "duration": 0.019932,
     "end_time": "2022-10-09T15:27:24.612586",
     "exception": false,
     "start_time": "2022-10-09T15:27:24.592654",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %%timeit\n",
    "# with torch.no_grad():\n",
    "#     saved_model(input)\n",
    "# torch.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "866678bb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-09T15:27:24.637968Z",
     "iopub.status.busy": "2022-10-09T15:27:24.637705Z",
     "iopub.status.idle": "2022-10-09T15:27:31.117202Z",
     "shell.execute_reply": "2022-10-09T15:27:31.116080Z"
    },
    "papermill": {
     "duration": 6.49535,
     "end_time": "2022-10-09T15:27:31.119935",
     "exception": false,
     "start_time": "2022-10-09T15:27:24.624585",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from zipfile import ZipFile\n",
    "with ZipFile('submission.zip','w') as zip_file:           \n",
    "    zip_file.write('./saved_model.pt', arcname='saved_model.pt') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d194ff80",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-09T15:27:31.149294Z",
     "iopub.status.busy": "2022-10-09T15:27:31.148946Z",
     "iopub.status.idle": "2022-10-09T15:27:31.154104Z",
     "shell.execute_reply": "2022-10-09T15:27:31.153230Z"
    },
    "papermill": {
     "duration": 0.022711,
     "end_time": "2022-10-09T15:27:31.156202",
     "exception": false,
     "start_time": "2022-10-09T15:27:31.133491",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !ls -lh"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 193.324903,
   "end_time": "2022-10-09T15:27:34.018583",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-10-09T15:24:20.693680",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
